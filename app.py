# app.py
from __future__ import annotations
import json
import httpx
from fastapi import Request
from fastapi.responses import StreamingResponse
import time
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional

from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama  # ä½ å½“å‰ç‰ˆæœ¬å¯ç”¨ï¼›æœ‰å¼ƒç”¨è­¦å‘Šä½†ä¸å½±å“è¿è¡Œ

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# ======================
# é…ç½®åŒº
# ======================
VECTORSTORE_DIR = "vectorstore"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

OLLAMA_MODEL = "qwen2.5:3b"  # æ”¹æˆä½  ollama list é‡Œå®é™…æ¨¡å‹å
OLLAMA_BASE_URL = "http://localhost:11434"

DEFAULT_TOP_K = 5

# ======================
# Embeddings / LLM
# ======================
def get_embeddings():
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

def get_llm():
    return Ollama(
        model=OLLAMA_MODEL,
        base_url=OLLAMA_BASE_URL,
        temperature=0.2,
    )

# ======================
# Prompt / æ–‡æ¡£æ ¼å¼åŒ–
# ======================
def format_docs(docs):
    blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "unknown")
        blocks.append(f"[{i}] æ¥æºï¼š{src}\n{d.page_content}")
    return "\n\n".join(blocks)

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæ³•å¾‹çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚
ä½ åªèƒ½ä¾æ®â€œå·²æ£€ç´¢åˆ°çš„æ³•æ¡/ææ–™â€å›ç­”ï¼Œä¸å¾—ç¼–é€ ï¼›è‹¥ä¾æ®ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜â€œæ— æ³•ä»…æ ¹æ®å½“å‰ææ–™ç¡®å®šâ€ï¼Œå¹¶æç¤ºéœ€è¦è¡¥å……æ¡ˆæƒ…æˆ–æ‰©å……æ³•è§„æ¥æºã€‚

è¾“å‡ºæ ¼å¼ï¼ˆåŠ¡å¿…éµå®ˆï¼‰ï¼š
ã€ç»“è®ºã€‘
ä¸€å¥è¯-ä¸‰å¥è¯ç»™å‡ºç»“è®ºã€‚

ã€æ³•å¾‹ä¾æ®ã€‘
ç”¨[1][2]è¿™æ ·çš„ç¼–å·å¼•ç”¨ææ–™ï¼Œå¹¶æ‘˜å½•å…³é”®å¥ï¼ˆä¸è¦ä¹±ç¼–æ¡å·ï¼›å¦‚æœææ–™é‡Œæ²¡æœ‰æ¡å·å°±ä¸è¦å†™æ¡å·ï¼‰ã€‚
"""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_PROMPT),
        ("human", "ç”¨æˆ·é—®é¢˜ï¼š{question}\n\nå·²æ£€ç´¢åˆ°çš„ææ–™ï¼š\n{context}\n\nè¯·è¾“å‡ºå›ç­”ï¼š"),
    ]
)

# ======================
# FastAPI Lifespan
# ======================
vs = None
chain = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global vs, chain

    embeddings = get_embeddings()
    vs = FAISS.load_local(
        VECTORSTORE_DIR,
        embeddings,
        allow_dangerous_deserialization=True,
    )

    retriever = vs.as_retriever(search_kwargs={"k": DEFAULT_TOP_K})
    llm = get_llm()

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
    )

    print("âœ… å‘é‡åº“ & Ollama åŠ è½½å®Œæˆ")
    yield
    print("ğŸ›‘ åº”ç”¨å…³é—­")

app = FastAPI(title="æ³•å¾‹çŸ¥è¯†é—®ç­”ç³»ç»Ÿï¼ˆChatUIï¼‰", lifespan=lifespan)

# é™æ€èµ„æºä¸æ¨¡æ¿
app.mount("/static", StaticFiles(directory="static"), name="static")

# ======================
# å‰ç«¯é¡µé¢
# ======================
@app.get("/", response_class=HTMLResponse)
def index():
    # ç®€å•èµ·è§ï¼Œç›´æ¥è¯» templates/index.html å†…å®¹è¿”å›ï¼ˆä¸ä¾èµ– jinja2ï¼‰
    with open("templates/index.html", "r", encoding="utf-8") as f:
        return f.read()

# ======================
# API Schema
# ======================
class ChatMessage(BaseModel):
    role: str  # "user" | "assistant"
    content: str

class ChatRequest(BaseModel):
    message: str
    history: Optional[List[ChatMessage]] = None
    top_k: int = DEFAULT_TOP_K

class ChatResponse(BaseModel):
    answer: str
    citations: List[Dict[str, Any]]
    latency_ms: int

# ======================
# Chat API
# ======================
@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    if vs is None or chain is None:
        return ChatResponse(answer="ç³»ç»Ÿå°šæœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•ã€‚", citations=[], latency_ms=0)

    t0 = time.time()

    # æ£€ç´¢ä¾æ®ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºå¼•ç”¨ï¼‰
    docs = vs.similarity_search(req.message, k=req.top_k)
    citations = [
        {
            "source": d.metadata.get("source", "unknown"),
            "snippet": d.page_content[:320].replace("\n", " ").strip(),
        }
        for d in docs
    ]

    # ç”Ÿæˆå›ç­”ï¼ˆRAGï¼‰
    answer = str(chain.invoke(req.message))

    latency_ms = int((time.time() - t0) * 1000)
    return ChatResponse(answer=answer, citations=citations, latency_ms=latency_ms)
@app.post("/chat_stream")
async def chat_stream(req: ChatRequest, request: Request):
    """
    SSE streaming:
    - event: token  æ¯ä¸ªå°ç‰‡æ®µ
    - event: meta   ç»“æŸæ—¶ä¸€æ¬¡æ€§è¿”å› citations + latency
    - event: done   æµç»“æŸ
    """
    if vs is None:
        return StreamingResponse(iter(["event: meta\ndata: {}\n\n"]), media_type="text/event-stream")

    t0 = time.time()

    # 1) æ£€ç´¢ citationsï¼ˆå…ˆç®—å‡ºæ¥ï¼Œæœ€åå‘ metaï¼‰
    docs = vs.similarity_search(req.message, k=req.top_k)
    citations = [
        {
            "source": d.metadata.get("source", "unknown"),
            "snippet": d.page_content[:320].replace("\n", " ").strip(),
        }
        for d in docs
    ]
    context = format_docs(docs)

    # 2) ç»„è£…ç»™ Ollama çš„ promptï¼ˆRAGï¼‰
    full_prompt = prompt.format_messages(question=req.message, context=context)
    # prompt.format_messages è¿”å›æ¶ˆæ¯å¯¹è±¡åˆ—è¡¨ï¼›æˆ‘ä»¬æŠŠå®ƒæ‹¼æˆæ–‡æœ¬æ›´é€šç”¨
    # system + human åˆæˆä¸€ä¸ªçº¯æ–‡æœ¬ prompt
    prompt_text = "\n\n".join([f"{m.type.upper()}:\n{m.content}" for m in full_prompt])

    async def event_gen():
        # SSE å»ºè®®å…ˆå‘ä¸€ä¸ªç©º token è®©å‰ç«¯è¿›å…¥â€œç”Ÿæˆä¸­â€
        yield "event: token\ndata: \n\n"

        try:
            async with httpx.AsyncClient(timeout=None) as client:
                # Ollama åŸç”Ÿæµå¼æ¥å£ï¼š/api/generate
                async with client.stream(
                    "POST",
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": OLLAMA_MODEL,
                        "prompt": prompt_text,
                        "stream": True,
                        "options": {"temperature": 0.2},
                    },
                ) as r:
                    r.raise_for_status()

                    async for line in r.aiter_lines():
                        if await request.is_disconnected():
                            return

                        if not line:
                            continue

                        data = json.loads(line)
                        # data["response"] æ˜¯å¢é‡ token
                        token = data.get("response", "")
                        if token:
                            # SSEï¼šæ¯æ¡æ•°æ®ä¸€è¡Œ data:
                            yield f"event: token\ndata: {json.dumps(token, ensure_ascii=False)}\n\n"

                        # done=true è¡¨ç¤º ollama ç»“æŸ
                        if data.get("done", False):
                            break

            latency_ms = int((time.time() - t0) * 1000)
            meta = {"citations": citations, "latency_ms": latency_ms}
            yield f"event: meta\ndata: {json.dumps(meta, ensure_ascii=False)}\n\n"
            yield "event: done\ndata: {}\n\n"

        except Exception as e:
            err = {"error": str(e)}
            yield f"event: meta\ndata: {json.dumps(err, ensure_ascii=False)}\n\n"
            yield "event: done\ndata: {}\n\n"

    return StreamingResponse(event_gen(), media_type="text/event-stream")